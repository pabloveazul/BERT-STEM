{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf810bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import num2words\n",
    "import re, string, timeit\n",
    "import random\n",
    "import glob\n",
    "import transformers\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from itertools import combinations\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d4ceb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f5148767fe44199201d09e43351461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/248k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e781fe4a1c4b0a8683485d68567f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/486k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ff41a78ada4deb975ac30e2d051534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/134 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000a09313f0449ab9a73b94b71aedace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/310 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d212d83cdf21460ab2af17d3b3022646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/650 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = transformers.BertTokenizerFast.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\",do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b16fe805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a34d343443647c4a1a8f71d860b6a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.BertForPreTraining.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a67f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_sentence(text):\n",
    "    \n",
    "    sentence_a = []\n",
    "    sentence_b = []\n",
    "    label = []\n",
    "    n_lines = len(text['sentence'])\n",
    "    for i, line in enumerate(text['sentence']):\n",
    "        # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "        if random.random() >= 0.5:\n",
    "            # this is IsNextSentence\n",
    "            sentence_a.append(line)\n",
    "            sentence_b.append(text['next_sentence'][i])\n",
    "            label.append(0)\n",
    "        else:\n",
    "            index = random.randint(0, n_lines-1)\n",
    "            # this is NotNextSentence\n",
    "            sentence_a.append(line)\n",
    "            sentence_b.append(text['next_sentence'][index])\n",
    "            label.append(1)\n",
    "    \n",
    "    return sentence_a, sentence_b, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "637e6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_for_mlm(inputs):\n",
    "    \n",
    "    # create random array of floats with equal dimensions to input_ids tensor\n",
    "    rand = torch.rand(inputs.input_ids.shape)\n",
    "    # create mask array\n",
    "    mask_arr = (rand < 0.15) * (inputs.input_ids != 4) * \\\n",
    "               (inputs.input_ids != 5) * (inputs.input_ids != 1)\n",
    "    selection = []\n",
    "\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        selection.append(\n",
    "            torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        )\n",
    "\n",
    "    for i in range(inputs.input_ids.shape[0]):\n",
    "        inputs.input_ids[i, selection[i]] = 0\n",
    "    \n",
    "    return(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d8823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inputs_from_text(text):\n",
    "    \n",
    "    sentence_a, sentence_b, label = generate_next_sentence(text)\n",
    "    \n",
    "    inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt',\n",
    "                   max_length=512, truncation=True, padding='max_length')\n",
    "    \n",
    "    inputs['next_sentence_label'] = torch.LongTensor([label]).T\n",
    "    inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "    inputs = mask_for_mlm(inputs)\n",
    "    \n",
    "    return(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4681ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2105b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_BertForPreTrainig(model, text):\n",
    "    \n",
    "    inputs = generate_inputs_from_text(text)\n",
    "    dataset = CorpusDataset(inputs)\n",
    "    \n",
    "    training_args = transformers.TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        num_train_epochs=3,              # total # of training epochs\n",
    "        per_device_train_batch_size=16,  # batch size per device during training\n",
    "        per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir='./logs')            # directory for storing logs\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,                         # the instantiated Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=dataset,               # training dataset\n",
    "        eval_dataset=dataset)                # evaluation dataset\n",
    "    \n",
    "    \n",
    "    return(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
